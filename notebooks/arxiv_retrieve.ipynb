{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "550b09e9-4536-4e00-bef9-eea854a2bde1",
   "metadata": {},
   "source": [
    "# ArxivHero\n",
    "This is a simple query + re-ranking engine, paired with a small ChatGPT-based summarizer + intro maker engine, that I created \n",
    "to be up to date with current research.\n",
    "\n",
    "## The system performs the following\n",
    "1. Retrieve recent arxiv papers according to query\n",
    "    * using `arxiv` library\n",
    "3.  find the topics of the retrieved papers\n",
    "    * using e.g., NMF + tf-Idf\n",
    "    * make intuitive subsets of the words describing the topics using further thresholding.\n",
    "4.  compute the relevance of all the papers to the keywords (using embedings)\n",
    "    * some heuristics with thresholding etc\n",
    "5.  discard papers and corresponding to the topics with low mean relevance\n",
    "6.  find the final relevant papers\n",
    "7.  creates a summary of the abstracts of these papers\n",
    "8.  Creates a title and introductory summary for the whole document\n",
    "9.  Collates the text created to a final document\n",
    "\n",
    "\n",
    "### TODO\n",
    "* Create some paper quality metrics (to be defined). Make a completely automated engine that pushes papers somewhere online where I can check them out later.\n",
    "* Do some simple OSINT on citations and authors.\n",
    "* Top-level parametrization (number of papers, query parameters etc)\n",
    "* Better HTML (inlcude more info)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7a9fbd4-cfae-43a7-9798-c13aa16f35e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%file requirements.txt\n",
    "\n",
    "arxiv\n",
    "langchain\n",
    "openai\n",
    "sentence-transformers\n",
    "transformers\n",
    "scikit-learn\n",
    "numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd24905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d4d0eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc86a416-b611-4b3c-9d48-b65a57dfcccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "## Special for this notebook - depends on relative paths:\n",
    "def _get_helper_css_js():\n",
    "    css = open('../src/assets/style.css','r').read()\n",
    "    js = open('../src/assets/script.js','r').read()\n",
    "    return css, js\n",
    "\n",
    "def _get_openai_api_key():\n",
    "    openai_api_key = open('../secret_openai_api_key.txt','r').read()[:-1]\n",
    "    return openai_api_key\n",
    "openai_api_key = _get_openai_api_key()\n",
    "\n",
    "\n",
    "css, js = _get_helper_css_js()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7c1c7614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = openai_api_key.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e08137eb-885c-48f0-a966-2e81f57e0cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "from scipy.sparse._csr import csr_matrix\n",
    "from typing import List, Dict, Tuple\n",
    "from abc import ABC\n",
    "\n",
    "## Some code to make pretty-printig HTML better:\n",
    "s_pre =  '<html>'\n",
    "s_pre += ' <head>'\n",
    "s_pre += '  <style>' + css + '</style>'\n",
    "s_pre += ' </head>'\n",
    "s_pre += ' <body>'\n",
    "## post:\n",
    "s_post = '</body>'\n",
    "s_post += '</html>'\n",
    "\n",
    "def collapsible_button_html(title, content, score):\n",
    "    if score is not None:\n",
    "        s  = '   <button class=\"collapsible\">(%2.3f) %s </button> '%(score, title)\n",
    "    else:\n",
    "        s  = '   <button class=\"collapsible\">%s </button> '%( title)\n",
    "    s += '   <div class=\"content\">'\n",
    "    s += '   <p>' + content + '</p>'\n",
    "    s += '   </div>'\n",
    "    return s\n",
    "\n",
    "def _make_collapsible(title, content, score = None):\n",
    "    s += s_pre\n",
    "    s += collapsible_button_html(title, content, score)\n",
    "    s += '<script>' + js + '</script>'\n",
    "    s += s_post\n",
    "    return s\n",
    "\n",
    "def _make_collapsibles(titles, content_list, scores_list):\n",
    "    s = s_pre\n",
    "    for k,(t, c) in enumerate(zip(titles, content_list)):\n",
    "        if scores_list is not None:\n",
    "            s += collapsible_button_html(t,c,scores_list[k])\n",
    "        else:\n",
    "            s += collapsible_button_html(t,c)\n",
    "    s += '<script>' + js +'</script>'\n",
    "    s += s_post\n",
    "    return s\n",
    "\n",
    "def _get_arxiv_papers_for_query(query, num_papers = 200):\n",
    "    \"\"\"\n",
    "    gets a list of arxiv papers according to some query\n",
    "    \"\"\"\n",
    "    \n",
    "    res = arxiv.Search(\n",
    "       interests_query,\n",
    "      id_list=[],\n",
    "      max_results = num_papers,\n",
    "      sort_by = arxiv.SortCriterion.SubmittedDate,\n",
    "      sort_order = arxiv.SortOrder.Descending\n",
    "    )\n",
    "    return res\n",
    "\n",
    "def _get_arxiv_paper_list_text_data(arxiv_res):\n",
    "    \"\"\"\n",
    "    Returns a list that contains the texts, given a list of arxiv results.\n",
    "    \"\"\"\n",
    "    res_text_dat = [];\n",
    "    query_results = []\n",
    "    for r in arxiv_res.results():\n",
    "        text_dat =  r.title +':\\n\\n' + r.summary\n",
    "        res_text_dat.append(text_dat)\n",
    "        query_results.append(r)\n",
    "    return res_text_dat, query_results\n",
    "\n",
    "\n",
    "def _topic_indices_from_topic_matrix(topic_matrix):\n",
    "    inds = []\n",
    "    for row in topic_matrix.T:\n",
    "        inds.append(np.where(row)[0])\n",
    "    return inds\n",
    "\n",
    "def _get_topics_nmf_tfidf(\n",
    "        texts,\n",
    "        ntopics = 15,\n",
    "        topic_accept_nmf_thresh = 0.1,\n",
    "        topic_rel_q_thresh = 0.75\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Gets a set of keywords using tf-idf, and simply non-negative matrix decomposition\n",
    "\n",
    "    Args:\n",
    "      texts  : a list of texts\n",
    "      ntopics : number of topics\n",
    "      topic_accept_nmf_thresh : the threshold above which to accept NMF components \n",
    "                      (the matrix is already usually sparse, but this helps making \n",
    "                      more intuitive sets of keywords for the papers)\\\n",
    "      \n",
    "    \"\"\"\n",
    "    tfidf = TfidfVectorizer(stop_words='english')\n",
    "    t = tfidf.fit_transform(texts)\n",
    "    rr = NMF(n_components=ntopics).fit_transform(t.T)\n",
    "    fnames = tfidf.get_feature_names_out()\n",
    "\n",
    "    # get words for topics:\n",
    "    topic_words = []\n",
    "    for r in rr.T:\n",
    "        topic_words.append(fnames[r>topic_accept_nmf_thresh])\n",
    "\n",
    "    topic_rel = t @ rr\n",
    "    q_v = np.quantile(topic_rel,topic_rel_q_thresh,1)\n",
    "    # pplot.pcolor(topic_rel.T > q_v)\n",
    "    topic_matrix = topic_rel.T>q_v\n",
    "    topic_indices = _topic_indices_from_topic_matrix(topic_matrix)\n",
    "    return  t, {'topic_indices' : topic_indices, 'topic_words' : topic_words ,'thresholded_topic_matrix' : topic_matrix}\n",
    "\n",
    "\n",
    "def _second_level_topic_selection(paper_query_rel_scores, topic_data, q_thresh_val = 0.80):\n",
    "    \"\"\"Further filtering of topics based on paper-query relevance\n",
    "\n",
    "    This creates a dictionary of topics that are relevant to the papers in the list.\n",
    "\n",
    "    The paper_query_rel_scores can be computed (for instance) as follows: \n",
    "    \n",
    "      `paper_query_rel_scores = emb_res @ enc_quer`\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    all_topic_inds = np.unique(np.stack(topic_data['topic_indices']))\n",
    "    top_rel_scores = {}\n",
    "    for topic_list, paper_score in zip(topic_data['topic_indices'], paper_query_rel_scores):\n",
    "        for t in topic_list:\n",
    "            if t not in top_rel_scores:\n",
    "                top_rel_scores[t] = [paper_score, 1]\n",
    "            else:\n",
    "                top_rel_scores[t][0] += paper_score\n",
    "                top_rel_scores[t][1] += 1\n",
    "                \n",
    "    for k in top_rel_scores.keys():\n",
    "        avg = top_rel_scores[k][0] / top_rel_scores[k][1]\n",
    "        top_rel_scores[k].append(avg)\n",
    "    \n",
    "    q_thresh = np.quantile([v[2] for k, v in top_rel_scores.items()], q_thresh_val)\n",
    "    topic_filter = {k : v[2] >= q_thresh for k, v in top_rel_scores.items()}\n",
    "    \n",
    "    #contains \"true\" when this is a topic to be kept.\n",
    "    kept_topics = {'indices' : [] , 'keywords' : []}\n",
    "    discarded_topics = {'indices' : [] , 'keywords' : []}\n",
    "    \n",
    "    for t, b in topic_filter.items():\n",
    "        if b:\n",
    "            kept_topics['indices'].append(t)\n",
    "            kept_topics['keywords'].append(topic_data['topic_words'][t])\n",
    "        else:\n",
    "            discarded_topics['indices'].append(t)\n",
    "            discarded_topics['keywords'].append(topic_data['topic_words'][t])\n",
    "\n",
    "    discarded_paper_inds, kept_paper_inds = [], []\n",
    "    for k,p in enumerate(topic_data['topic_indices']):\n",
    "        is_in_kept = False\n",
    "        for i in p:\n",
    "            if i in kept_topics['indices']:\n",
    "                kept_paper_inds.append(k)\n",
    "                is_in_kept = True\n",
    "                break\n",
    "                \n",
    "        if not is_in_kept:\n",
    "            discarded_paper_inds.append(k)\n",
    "        \n",
    "    return (kept_paper_inds, discarded_paper_inds), (kept_topics, discarded_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2569299f-5984-435d-b0fe-d726f1da9a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TopicModeler(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, v : List[str]):\n",
    "        pass\n",
    "        \n",
    "class TFIDFNMFTopicModeler(TopicModeler):\n",
    "    def __init__(\n",
    "            self,\n",
    "            ntopics = 5,\n",
    "            topic_accept_nmf_thresh = 0.1,\n",
    "            topic_rel_q_thresh = 0.75\n",
    "        ):\n",
    "        self.ntopics = ntopics\n",
    "        self.topic_accept_nmf_thresh = topic_accept_nmf_thresh\n",
    "        self.topic_rel_q_thresh = topic_rel_q_thresh\n",
    "    \n",
    "    def __call__(self, vals : List[str]) -> [csr_matrix, Dict] :\n",
    "        return _get_topics_nmf_tfidf(\n",
    "            vals,\n",
    "            ntopics=self.ntopics,\n",
    "            topic_accept_nmf_thresh=self.topic_accept_nmf_thresh,\n",
    "            topic_rel_q_thresh=self.topic_rel_q_thresh\n",
    "        )\n",
    "        \n",
    "class ArxivCustomRetrieval:\n",
    "    \"\"\"\n",
    "    A hand-engineered retrieval engine, that \n",
    "    performs simple topic modeling and simple inner-product embedings-based\n",
    "    topic and paper relevance determination (for filtering the most relevant papers)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            topic_modeler : TopicModeler, \n",
    "            q_topic_thresh_val = 0.8,\n",
    "            num_papers_query = 100,\n",
    "            top_n_relevant = 10,\n",
    "            embeding_model_str = 'thenlper/gte-base', **kwargs\n",
    "        ):\n",
    "        \"\"\"\n",
    "        A simple stateful wrapper to all the utility functions.\n",
    "\n",
    "        Args:\n",
    "\n",
    "            topic_modeler: an object that can return topics from text (in a speciffic format)\n",
    "            topic_2nd_lv_quantile_thresh : after preliminary ID of the topics, using the embeddings of the retrieved texts, \n",
    "                           the IDed topics are re-evaluated for matching the initial query. This value is used to compute \n",
    "                           quantiles of topic-matching values (computed as the average of relevance score of topic-related\n",
    "                           papers. \n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        self.topic_modeler = topic_modeler\n",
    "        self.num_papers_query = num_papers_query\n",
    "        model = SentenceTransformer(embeding_model_str)\n",
    "        self.embedding_model = model\n",
    "        self.q_topic_thresh_val = q_topic_thresh_val\n",
    "        self.docs_embedded = False\n",
    "        self.arxiv_papers_retrieved = False\n",
    "        self._top_n_relevant = top_n_relevant # controls how many of the papers are actually printed/used\n",
    "\n",
    "    def get_full_state(self):\n",
    "        query_state = (self.text_res, self.query_res)\n",
    "        embedding_state = (self._enc_quer, self._emb_res ,self._paper_query_rel_scores, self.docs_embedded)\n",
    "        topic_state = (self.topic_matrix, self.topic_data)\n",
    "        return {'topic_state' : topic_state, 'embedding_state' : embedding_state, 'query_state' : query_state} \n",
    "        \n",
    "    def set_full_state(self, state):\n",
    "        self.topic_matrix, self.topic_data = state['topic_state']\n",
    "        self._enc_quer, self._emb_res, self._paper_query_rel_scores, self.docs_embedded  = state['embedding_state']\n",
    "        self.text_res, self.query_res = state['query_state']\n",
    "        \n",
    "        \n",
    "    def run(self, query):\n",
    "        \"\"\"\n",
    "        Full run:\n",
    "         1. running the retrieval from arxiv\n",
    "         2. performing topic modeling with TFIDF and NMF \n",
    "         3. reducing the topics to the most discriminative (through thresholding)\n",
    "         4. embeding the retrieved documents (using a transformer model)\n",
    "         5. computing the relevance of the retrieved documents with the provided query\n",
    "         6. finding the mean topic relevance (by summing per-\n",
    "           topic instance how relevant the documents that were assigned that topic are)\n",
    "         7. setting (through thresholding) the most relevant topics\n",
    "         8. discarding/keeping according to topic relevance the corresponding papers\n",
    "        \"\"\"\n",
    "        # 1. \n",
    "        self.execute_query(query)\n",
    "        # 2. + 3.\n",
    "        self.get_topic_data(self.text_res)\n",
    "        # 4. + 5. \n",
    "        self.embed_docs(self.text_res, query)\n",
    "        # paper_query_rel_scores = self._emb_res @ self._enc_quer\n",
    "        # 6. 7. 8. \n",
    "        (kept_paper_inds, discarded_paper_inds), (kept_topics, discarded_topics) = _second_level_topic_selection(\n",
    "            self._paper_query_rel_scores, \n",
    "            self.topic_data,\n",
    "            q_thresh_val = self.q_topic_thresh_val\n",
    "        )\n",
    "        \n",
    "        self.kept_paper_inds, self.discarded_paper_inds = kept_paper_inds, discarded_paper_inds\n",
    "        self.kept_topics, self. discarded_topics = kept_topics, discarded_topics        \n",
    "\n",
    "    def get_kept_papers_text(self):\n",
    "        return [self.text_res[k] for k in self.kept_paper_inds]\n",
    "\n",
    "    def get_kept_papers_results(self):\n",
    "        return [self.query_res[k] for k in self.kept_paper_inds]\n",
    "        \n",
    "    def get_kept_paper_arxiv_query_res(self):\n",
    "        return [self.query_res[k] for k in self.kept_paper_inds]\n",
    "\n",
    "    def embed_docs(self, text_res, interests_query):\n",
    "        if not self.docs_embedded:\n",
    "            emb_res = self.embedding_model.encode(text_res)\n",
    "            enc_quer = self.embedding_model.encode(interests_query)\n",
    "            self._enc_quer = enc_quer\n",
    "            self._emb_res = emb_res\n",
    "            self._paper_query_rel_scores = emb_res @ enc_quer\n",
    "            self.docs_embedded = True\n",
    "\n",
    "    def get_topic_data(self, text_list : List[str]):\n",
    "        \"\"\"\n",
    "        Creates a set of topics (as a sparse matrix and a vector of topic indices)\n",
    "        \n",
    "        See also:\n",
    "          `TFIDF_NMF_TopicModeler`\n",
    "        \"\"\"\n",
    "        self.topic_matrix, self.topic_data = self.topic_modeler(text_list)\n",
    "        \n",
    "    def execute_query(self, query):\n",
    "        \"\"\"\n",
    "        Executes the query and stores the results in the object.\n",
    "        \"\"\"\n",
    "        res = _get_arxiv_papers_for_query(query, num_papers=self.num_papers_query)\n",
    "        text_res, query_res = _get_arxiv_paper_list_text_data(res)\n",
    "        self.text_res, self.query_res = text_res, query_res\n",
    "        \n",
    "    def get_most_relevant_inds(self, top_n = None):\n",
    "        if top_n is None:\n",
    "            top_n = self._top_n_relevant\n",
    "        return np.argsort(-self._paper_query_rel_scores)[:top_n]\n",
    "        \n",
    "    def _repr_html_(self):\n",
    "        \"\"\"returns HTML ready to render or add as a component, that contains title and abstract from the arxiv papers.\n",
    "        \"\"\"\n",
    "        argsort_scores = np.argsort(-self._paper_query_rel_scores)\n",
    "        query_results = self.query_res\n",
    "        titles, contents, scores = [], [], []\n",
    "        for idx in argsort_scores[:self._top_n_relevant]:\n",
    "            q = self.query_res[idx]\n",
    "            score = self._paper_query_rel_scores[idx]\n",
    "            titles.append(q.title)\n",
    "            contents.append(q.summary)\n",
    "            scores.append(score)\n",
    "            \n",
    "        s = _make_collapsibles(titles, contents, scores)\n",
    "        return s\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "edcd50d9-bc2c-46b9-a1d3-5fa927317791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3d4c8b2d-ac5b-499e-bcfe-ebbfba4b2dc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, OpenAI, LLMChain\n",
    "\n",
    "def _make_html_from_query_res(query_res, summary):\n",
    "    entry_id = query_res.entry_id\n",
    "    s = ''\n",
    "    s += '<div><h4><a href=' + entry_id + '>' + query_res.title\n",
    "    s += ' (' + entry_id.split('/')[-1] + ')'  + '</a> </h4><div>'\n",
    "    s += '<p>' + summary + '</p>'\n",
    "    return s\n",
    "\n",
    "class DocGenerationEngine:\n",
    "    def __init__(self, search_results):\n",
    "        # def llm_summarizer(paper_text):\n",
    "\n",
    "        self.search_results = search_results\n",
    "        \n",
    "        self.summary_focus_summ = '''\n",
    "            potential immediate enterprise use cases, as well as the claimed benefits of the original innovations.\n",
    "            '''\n",
    "        prompt_template_summ = \"\"\"\n",
    "            Write a very short (around 50 words) summary\n",
    "            of the following paper title and abstract. Focus on {summary_focus} of the described techniques.:\\n {paper_text}\n",
    "            \"\"\"\n",
    "        \n",
    "        prompt_template_intro = '''\n",
    "            Write a short and engaging title (around 30 words) and introduction \n",
    "            (around 200 words) for a document that presents recent machine learning research news.\n",
    "            Focus the intro on potential breakthroughs from the presented research. \n",
    "            \n",
    "            Here is the text to use as reference:\n",
    "            {input_text}\n",
    "        \n",
    "            wrap the title in an HTML <h1> tag and the intro in a <p> tag.\n",
    "            Begin!\n",
    "            '''\n",
    "        \n",
    "        self.prompt_intro = PromptTemplate.from_template(prompt_template_intro)\n",
    "        self.prompt_summarization = PromptTemplate.from_template(prompt_template_summ)\n",
    "        \n",
    "        self.llm = OpenAI(temperature=0.1)\n",
    "        \n",
    "        self.llm_chain_summarizer = LLMChain(\n",
    "            llm=self.llm,\n",
    "            prompt=self.prompt_summarization,\n",
    "        )\n",
    "        self.llm_chain_intro_maker = LLMChain(\n",
    "            llm = self.llm, \n",
    "            prompt = self.prompt_intro\n",
    "        )\n",
    "        self.paper_summaries = None\n",
    "        self.intro = None\n",
    "    \n",
    "    def _paper_abstract_summarizer(self,p):\n",
    "        # def llm_summarizer(paper_text):\n",
    "        return self.llm_chain_summarizer.run(summary_focus = self.summary_focus_summ, paper_text = p)\n",
    "    \n",
    "    def _document_intro_maker(self, paper_titles_concat):\n",
    "        return self.llm_chain_intro_maker.run(input_text = paper_titles_concat)\n",
    "\n",
    "    def make_document(self):\n",
    "        from tqdm import tqdm\n",
    "        ## Create very brief summaries for each paper:\n",
    "        paper_summaries = []\n",
    "        \n",
    "        sr = self.search_results\n",
    "        most_rel_query_res = [sr.query_res[k] for k in sr.get_most_relevant_inds()]\n",
    "        most_rel_text_res = [sr.text_res[k] for k in sr.get_most_relevant_inds()]\n",
    "        if self.paper_summaries is None:\n",
    "            for p in tqdm(most_rel_text_res):\n",
    "                paper_summaries.append(self._paper_abstract_summarizer(p))\n",
    "            self.paper_summaries = paper_summaries\n",
    "            \n",
    "        if self.intro is None:\n",
    "            self.paper_summaries_for_render =  [_make_html_from_query_res(q, summ) for q, summ in zip(most_rel_query_res, self.paper_summaries)]\n",
    "            self.intro = self._document_intro_maker(''.join(paper_summaries))\n",
    "            \n",
    "        return self.intro + '\\n'.join(self.paper_summaries_for_render)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c56be11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96708b16-c8eb-40e0-8ce4-633c3846c5bc",
   "metadata": {},
   "source": [
    "-----\n",
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f59d12a6-2be7-40d4-b093-0275759e6cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6066/3845215456.py:69: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for r in arxiv_res.results():\n",
      "100%|██████████| 10/10 [00:12<00:00,  1.22s/it]\n"
     ]
    }
   ],
   "source": [
    "interests_query = \"llm chatgpt efficient inference\"\n",
    "sr = ArxivCustomRetrieval(topic_modeler=TFIDFNMFTopicModeler(), q_topic_thresh_val=0.5, top_n_relevant=10)\n",
    "sr.run(interests_query)\n",
    "d = DocGenerationEngine(sr)\n",
    "doc = d.make_document()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c0b93111-b840-4970-a123-ea4e64b62a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<h1>Unlocking the Potential of Machine Learning Research: Recent Breakthroughs</h1>\n",
       "\n",
       "<p>The potential of machine learning research is vast and ever-growing. Recent breakthroughs in the field have enabled faster and more efficient inference for enterprise use cases such as natural language processing and machine translation. New benchmarks have been proposed to accurately assess the performance of chatbots, and frameworks have been developed to accelerate the benchmarking of large language models (LLMs) for NLP tasks. Transferring reasoning capabilities from large language models to smaller models has been made possible, and new methods have been proposed to enhance AI systems' emotional reasoning with commonsense knowledge. Open-source LLMs have been compared to GPT-4 and Claude 2 on multiple-choice questions in Nephrology, and a unified and customized instruction-following language model has been proposed for e-commerce authoring tasks. Finally, a language model specifically designed to critique and refine language model outputs has been developed, and the use of ChatGPT for code generation has been evaluated. </p>\n",
       "\n",
       "<p>These recent breakthroughs in machine learning research have the potential to revolutionize the way enterprises use AI. Faster and more efficient inference can enable faster and more accurate natural<div><h4><a href=http://arxiv.org/abs/2308.04623v1>Accelerating LLM Inference with Staged Speculative Decoding (2308.04623v1)</a> </h4><div><p>\n",
       "This paper proposes a novel algorithm, staged speculative decoding, to accelerate LLM inference in small-batch, on-device scenarios. It reduces single-batch decoding latency by 3.16x while perfectly preserving output quality, enabling faster and more efficient LLM inference for enterprise use cases such as natural language processing and machine translation.</p>\n",
       "<div><h4><a href=http://arxiv.org/abs/2308.04624v1>Benchmarking LLM powered Chatbots: Methods and Metrics (2308.04624v1)</a> </h4><div><p>\n",
       "This paper proposes a novel benchmark, the E2E benchmark, to accurately assess the performance of chatbots, especially those powered by Large Language Models (LLMs). The proposed benchmark uses cosine similarity to evaluate accuracy and usefulness of the answers provided by chatbots, and shows better results compared to other available metrics. Enterprises can use this benchmark to evaluate their chatbots and benefit from the improved accuracy and usefulness of the answers.</p>\n",
       "<div><h4><a href=http://arxiv.org/abs/2308.04945v1>LLMeBench: A Flexible Framework for Accelerating LLMs Benchmarking (2308.04945v1)</a> </h4><div><p>\n",
       "LLMeBench is a flexible framework for accelerating the benchmarking of Large Language Models (LLMs) for NLP tasks across different languages. It features zero- and few-shot learning settings, and can be customized for any task and model in less than 10 minutes. It has been tested on 31 tasks and 53 datasets, and is open-sourced for the community.</p>\n",
       "<div><h4><a href=http://arxiv.org/abs/2308.04679v1>Sci-CoT: Leveraging Large Language Models for Enhanced Knowledge Distillation in Small Models for Scientific QA (2308.04679v1)</a> </h4><div><p>\n",
       "Sci-CoT is a two-stage framework that enables the transfer of reasoning capabilities from large language models to smaller models for scientific question-answering tasks. It separates the processes of generating rationales and inferring answers, leading to improved performance with fewer computational resources. This could be used in enterprise settings to improve the accuracy of QA systems while reducing the computational costs.</p>\n",
       "<div><h4><a href=http://arxiv.org/abs/2308.04813v1>CLEVA: Chinese Language Models EVAluation Platform (2308.04813v1)</a> </h4><div><p>\n",
       "CLEVA is a Chinese Language Models EVAluation Platform that provides a standardized workflow to holistically evaluate Chinese LLMs. It curates new data to reduce contamination and provides an easy-to-use interface to minimize coding. Enterprises can use CLEVA to thoroughly evaluate Chinese LLMs and benefit from its competitive leaderboard and sampling strategy.</p>\n",
       "<div><h4><a href=http://arxiv.org/abs/2308.04811v1>A Bipartite Graph is All We Need for Enhancing Emotional Reasoning with Commonsense Knowledge (2308.04811v1)</a> </h4><div><p>\n",
       "This paper proposes a Bipartite Heterogeneous Graph (BHG) method for enhancing AI systems' emotional reasoning with commonsense knowledge. The BHG method can be generalized to multi-type and multi-grained knowledge sources, and the proposed Multi-dimensional Heterogeneous Graph Transformer (MHGT) can retain unchanged feature spaces and unequal dimensions for heterogeneous node types. This method can be used in applications such as online opinion mining from social media and empathetic dialogue systems, and experiments show that it significantly outperforms existing methods.</p>\n",
       "<div><h4><a href=http://arxiv.org/abs/2308.04709v1>A Comparative Study of Open-Source Large Language Models, GPT-4 and Claude 2: Multiple-Choice Test Taking in Nephrology (2308.04709v1)</a> </h4><div><p>\n",
       "This study compared the performance of open-source large language models (LLMs) to GPT-4 and Claude 2 on multiple-choice questions in Nephrology. The results showed that open-source LLMs performed poorly, with an overall success rate of 17.1-25.5%. In contrast, Claude 2 and GPT-4 achieved 54.4% and 73.3% accuracy respectively. These findings suggest that LLMs have potential applications in medical training and patient care, and could be used to improve accuracy and efficiency in these areas.</p>\n",
       "<div><h4><a href=http://arxiv.org/abs/2308.04913v1>LLaMA-E: Empowering E-commerce Authoring with Multi-Aspect Instruction Following (2308.04913v1)</a> </h4><div><p>\n",
       "This paper proposes LLaMA-E, a unified and customized instruction-following language model for e-commerce authoring tasks. It enables domain experts to create seed instructions for tasks such as ads generation, query-enhanced product title rewriting, product classification, purchase intent speculation, and general Q&A. The proposed model achieves state-of-the-art results in quantitative and qualitative evaluations, and offers potential immediate enterprise use cases with the claimed benefits of improved customer service, enhanced product titles, and better product classification.</p>\n",
       "<div><h4><a href=http://arxiv.org/abs/2308.04592v1>Shepherd: A Critic for Language Model Generation (2308.04592v1)</a> </h4><div><p>\n",
       "Shepherd is a language model specifically designed to critique and refine language model outputs. It leverages a high quality feedback dataset and is small (7B parameters). It has been evaluated against established models and outperforms them, with an average win-rate of 53-87%. It has potential immediate enterprise use cases, such as providing feedback on customer service responses and suggesting refinements.</p>\n",
       "<div><h4><a href=http://arxiv.org/abs/2308.04838v1>No Need to Lift a Finger Anymore? Assessing the Quality of Code Generation by ChatGPT (2308.04838v1)</a> </h4><div><p>\n",
       "This paper evaluates the use of ChatGPT, a large language model, for code generation. Results show that ChatGPT can generate code that is correct, understandable, and secure. Additionally, it can engage in multi-round processes to facilitate code generation. This research provides valuable insights into the performance of ChatGPT for code generation, and can be used to improve AI-based code generation techniques for enterprise use cases.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
