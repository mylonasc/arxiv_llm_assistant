<html><head></head><html><head><style>
/* Style the button that is used to open and close the collapsible content */
.collapsible {
  background-color: #eee;
  color: #444;
  cursor: pointer;
  padding: 18px;
  width: 100%;
  border: none;
  text-align: left;
  outline: none;
  font-size: 15px;
}

/* Add a background color to the button if it is clicked on (add the .active class with JS), and when you move the mouse over it (hover) */
.active, .collapsible:hover {
  background-color: #ccc;
}

/* Style the collapsible content. Note: hidden by default */
.content {
  padding: 0 18px;
  border: 5px ;
  margin: auto;
  width: 50%;
  padding: 10px;
}
</style></head><body><div class="intro content">
<h1>Recent Developments in Machine Learning Research</h1>
<p>Welcome to our newsletter, where we bring you the latest breakthroughs in machine learning research. In this edition, we will be exploring the potential impact of various techniques and models, such as GRITv2, Large Language Models, Graph Neural Networks, and more, in academic research. These advancements have the potential to greatly enhance the efficiency and effectiveness of various tasks, from social relation recognition to legal document classification. Join us as we dive into the exciting world of machine learning and its potential for groundbreaking discoveries.</p> </div><div class="content">
<h4><a href=http://arxiv.org/abs/2403.06895v1>GRITv2: Efficient and Light-weight Social Relation Recognition (2403.06895v1)</a> </h4>
<p>
The paper "GRITv2: Efficient and Light-weight Social Relation Recognition" presents a new state-of-the-art relation recognition model, GRITv2, which surpasses existing methods on the PISC relation dataset. The model has been improved in terms of efficiency and performance, with a smaller version, GRITv2-S, achieving similar results to the larger version, GRITv2-L, with significantly fewer parameters. The model has also been successfully deployed on a mobile device, highlighting its practical viability and potential for efficient use in academic research.</p>
</div>
<div class="content">
<h4><a href=http://arxiv.org/abs/2403.06949v1>Materials science in the era of large language models: a perspective (2403.06949v1)</a> </h4>
<p>
This paper discusses the potential impact of Large Language Models (LLMs) in materials science research. LLMs have shown impressive natural language capabilities and can handle ambiguous requirements, making them versatile tools for various tasks and disciplines. The paper provides two case studies demonstrating the use of LLMs in task automation and knowledge extraction. The authors argue that LLMs should be viewed as tireless workers that can accelerate and unify exploration across domains, rather than oracles of novel insight. This paper aims to familiarize material science researchers with the concepts needed to leverage LLMs in their own research.</p>
</div>
<div class="content">
<h4><a href=http://arxiv.org/abs/2403.06872v1>Exploring Large Language Models and Hierarchical Frameworks for Classification of Large Unstructured Legal Documents (2403.06872v1)</a> </h4>
<p>
This paper explores the potential of using large language models and hierarchical frameworks for classification of large unstructured legal documents. The proposed framework, MESc, utilizes deep learning and unsupervised clustering to predict judgments from documents without structural annotation. The study also analyzes the adaptability and transfer learning capabilities of large language models, showing a significant performance gain over previous methods. These techniques have the potential to greatly impact the field of legal judgment prediction in academic research.</p>
</div>
<div class="content">
<h4><a href=http://arxiv.org/abs/2403.06817v1>Are Targeted Messages More Effective? (2403.06817v1)</a> </h4>
<p>
This paper explores the potential impact of graph neural networks (GNN) in academic research. GNNs are deep learning architectures for graphs that operate on vertices through message passing. The paper compares two versions of GNNs and their expressivity in terms of first-order logic with counting. It concludes that while the two versions have the same expressivity in a non-uniform setting, the second version is more expressive in a uniform setting. This finding has the potential to enhance the use of GNNs in academic research.</p>
</div>
<div class="content">
<h4><a href=http://arxiv.org/abs/2403.06925v1>Simplicity Bias of Transformers to Learn Low Sensitivity Functions (2403.06925v1)</a> </h4>
<p>
This paper explores the simplicity bias of transformers, a popular neural network architecture, and how it differs from other architectures. The authors identify sensitivity to random changes in input as a measure of simplicity bias and show that transformers have lower sensitivity compared to other architectures. This low-sensitivity bias is also linked to improved robustness, making it a valuable tool for enhancing the performance of transformers in various tasks.</p>
</div>
<div class="content">
<h4><a href=http://arxiv.org/abs/2403.06914v1>MEND: Meta dEmonstratioN Distillation for Efficient and Effective In-Context Learning (2403.06914v1)</a> </h4>
<p>
MEND is a novel technique that allows large language models to efficiently and effectively learn in-context by distilling lengthy demonstrations into compact vectors. It utilizes meta-knowledge and knowledge distillation to achieve both efficiency and effectiveness without the need for task-specific retraining. Comprehensive evaluations show that MEND outperforms other state-of-the-art distillation models, promising enhanced scalability and efficiency for the practical deployment of large language models in academic research.</p>
</div>
<div class="content">
<h4><a href=http://arxiv.org/abs/2403.06935v1>Naming, Describing, and Quantifying Visual Objects in Humans and LLMs (2403.06935v1)</a> </h4>
<p>
This paper explores the ability of Vision & Language Large Language Models (VLLMs) to mimic the distribution of plausible labels used by humans when describing visual objects. The study focuses on the potential impact of these techniques in academic research, particularly for uncommon or novel objects where a category label may be lacking. Results show mixed evidence, highlighting the need for further exploration and development in this area.</p>
</div>
<div class="content">
<h4><a href=http://arxiv.org/abs/2403.06970v1>MRL Parsing Without Tears: The Case of Hebrew (2403.06970v1)</a> </h4>
<p>
The paper presents a new "flipped pipeline" approach for syntactic parsing in morphologically rich languages, using Hebrew as a test case. This approach, which involves expert classifiers making decisions directly on whole-token units, sets a new state-of-the-art in Hebrew POS tagging and dependency parsing. It also has the potential to be applied to other MRLs, making it a valuable tool for future research in resource-scarce languages.</p>
</div>
<div class="content">
<h4><a href=http://arxiv.org/abs/2403.06932v1>ERA-CoT: Improving Chain-of-Thought through Entity Relationship Analysis (2403.06932v1)</a> </h4>
<p>
The paper presents a novel approach, ERA-CoT, that improves the performance of large language models (LLMs) in understanding complex scenarios involving multiple entities. The proposed method captures relationships between entities and supports reasoning through Chain-of-Thoughts (CoT). Experimental results show a significant improvement in LLMs' understanding of entity relationships, accuracy of question answering, and reasoning ability. This has the potential to create a lasting impact in academic research on LLMs and their applications in natural language processing tasks.</p>
</div>
<div class="content">
<h4><a href=http://arxiv.org/abs/2403.06832v1>The Power of Noise: Toward a Unified Multi-modal Knowledge Graph Representation Framework (2403.06832v1)</a> </h4>
<p>
This paper presents a novel approach, called SNAG, for Multi-modal Knowledge Graph representation learning. By incorporating specific training objectives for two widely researched tasks, SNAG achieves state-of-the-art performance on ten datasets. This framework has the potential to significantly improve the integration of structured knowledge into multi-modal Large Language Models, addressing issues such as knowledge misconceptions and multi-modal hallucinations. Its availability as open-source code and data also allows for further research and development in this area.</p>
</div></body></html><body></body></html>