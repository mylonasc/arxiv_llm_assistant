<html><head></head><html><head><style>
/* Style the button that is used to open and close the collapsible content */
.collapsible {
  background-color: #eee;
  color: #444;
  cursor: pointer;
  padding: 18px;
  width: 100%;
  border: none;
  text-align: left;
  outline: none;
  font-size: 15px;
}

/* Add a background color to the button if it is clicked on (add the .active class with JS), and when you move the mouse over it (hover) */
.active, .collapsible:hover {
  background-color: #ccc;
}

/* Style the collapsible content. Note: hidden by default */
.content {
  padding: 0 18px;
  border: 5px ;
  margin: auto;
  width: 50%;
  padding: 10px;
}
</style></head><body><div class="intro content">
<h1>Recent Developments in Machine Learning Research</h1>

<p>Welcome to our newsletter, where we bring you the latest and most exciting developments in the world of machine learning research. In this issue, we will be highlighting potential breakthroughs from recent papers that have the potential to greatly impact academic research in various fields. From improving the efficiency and effectiveness of large language models to enhancing low-resource language analysis and self-improvement of LLM agents, these papers showcase the constant evolution and innovation in the field of machine learning. So, let's dive in and explore the potential of these groundbreaking techniques to shape the future of machine learning research.</p> </div><div class="content">
<h4><a href=http://arxiv.org/abs/2405.20202v1>One QuantLLM for ALL: Fine-tuning Quantized LLMs Once for Efficient Deployments (2405.20202v1)</a> </h4>
<p>
This paper presents a new approach for efficiently deploying large language models (LLMs) by training a single supernet that can generate optimal subnets for different applications. By decoupling shared weights and incorporating Low-Rank adapters, the interference and training time are reduced. A non-parametric scheduler is also introduced to balance the allocation of resources for different subnets. This technique has the potential to significantly improve the efficiency and effectiveness of LLMs in academic research.</p>
</div>
<div class="content">
<h4><a href=http://arxiv.org/abs/2405.20339v1>Visual Perception by Large Language Model's Weights (2405.20339v1)</a> </h4>
<p>
This paper presents a novel approach for incorporating visual information into Large Language Models (LLMs) by using perceptual weights instead of visual tokens. This parameter space alignment paradigm reduces the computational effort and improves efficiency, while achieving comparable performance on various vision-language tasks. The proposed technique has the potential to significantly impact academic research by reducing the computational costs for training and inference in MLLMs.</p>
</div>
<div class="content">
<h4><a href=http://arxiv.org/abs/2405.20247v1>KerasCV and KerasNLP: Vision and Language Power-Ups (2405.20247v1)</a> </h4>
<p>
The paper introduces KerasCV and KerasNLP, extensions of the Keras API for Computer Vision and Natural Language Processing workflows. These domain packages offer fast experimentation, ease-of-use, and performance, with a modular and layered design. They provide building blocks for creating models and data preprocessing pipelines, as well as pretrained "task" models for popular architectures. These libraries have the potential to greatly enhance and streamline academic research in the fields of computer vision and natural language processing.</p>
</div>
<div class="content">
<h4><a href=http://arxiv.org/abs/2405.20192v1>TAIA: Large Language Models are Out-of-Distribution Data Learners (2405.20192v1)</a> </h4>
<p>
The paper presents a new method, called TAIA, for improving the performance of large language models (LLMs) in data-scarce domains with domain-mismatched data. By re-evaluating the Transformer architecture, the authors discovered that only fine-tuned attention parameters are particularly beneficial in these scenarios. Their proposed method, \trainallInfAttn, achieves superior improvements compared to traditional fine-tuning methods, making it resistant to jailbreaking tuning and enhancing specialized tasks using general data. This has the potential to create a lasting impact in academic research by providing a more effective and efficient way to improve LLM performance in data-scarce domains.</p>
</div>
<div class="content">
<h4><a href=http://arxiv.org/abs/2405.20180v1>Transformers and Slot Encoding for Sample Efficient Physical World Modelling (2405.20180v1)</a> </h4>
<p>
This paper presents a new architecture that combines Transformers and slot-attention paradigm for world modelling from video input. The proposed approach shows significant improvements in sample efficiency and performance consistency compared to existing solutions. This has the potential to greatly impact academic research in the field of world modelling, as it offers a more efficient and accurate way to build representations of the physical world.</p>
</div>
<div class="content">
<h4><a href=http://arxiv.org/abs/2405.20194v1>Occam Gradient Descent (2405.20194v1)</a> </h4>
<p>
The paper presents Occam Gradient Descent, an algorithm that balances the competing demands of deep learning models by simultaneously reducing model size and minimizing fitting error. This approach is more efficient and effective than traditional gradient descent, as shown in experiments. The potential for this technique to improve accuracy, reduce computing resources, and compress models could have a lasting impact on academic research in deep learning.</p>
</div>
<div class="content">
<h4><a href=http://arxiv.org/abs/2405.20145v1>Heidelberg-Boston @ SIGTYP 2024 Shared Task: Enhancing Low-Resource Language Analysis With Character-Aware Hierarchical Transformers (2405.20145v1)</a> </h4>
<p>
This paper presents a submission to the SIGTYP 2024 shared task, focusing on enhancing low-resource language analysis for historical languages. By utilizing character-aware hierarchical transformers and character-level T5 models, the authors were able to achieve first place in the constrained subtask and demonstrate the potential for these techniques to improve research in this field.</p>
</div>
<div class="content">
<h4><a href=http://arxiv.org/abs/2405.20309v1>Large Language Models Can Self-Improve At Web Agent Tasks (2405.20309v1)</a> </h4>
<p>
This paper explores the potential for large language models (LLMs) to self-improve their performance as agents in complex environments, specifically in the WebArena benchmark. By fine-tuning on synthetic training data, the LLMs were able to achieve a 31% improvement in task completion rate. This research also introduces new evaluation metrics for assessing the quality and capabilities of the fine-tuned LLM agents. These techniques have the potential to greatly impact and improve the use of LLMs in academic research for agent-based tasks.</p>
</div>
<div class="content">
<h4><a href=http://arxiv.org/abs/2405.20215v1>TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models (2405.20215v1)</a> </h4>
<p>
The paper presents a new framework, TS-Align, for aligning large language models (LLMs) that reduces the reliance on human preference data and improves scalability. Through the collaboration between a large-scale teacher model and a small-scale student model, the framework automatically mines pairwise feedback data to fine-tune the policy model. The experiments show that the final aligned policy outperforms the base model, demonstrating the potential for lasting impact in improving the efficiency and effectiveness of LLM alignment in academic research.</p>
</div>
<div class="content">
<h4><a href=http://arxiv.org/abs/2405.20335v1>Xwin-LM: Strong and Scalable Alignment Practice for LLMs (2405.20335v1)</a> </h4>
<p>
Xwin-LM is a suite of alignment techniques for large language models that includes supervised finetuning, reward modeling, rejection sampling finetuning, and direct preference optimization. These techniques have the potential to significantly improve the performance of language models in academic research, as demonstrated by consistent and significant improvements in evaluations. The open-source repository also encourages further community research in this area.</p>
</div></body></html><body></body></html>