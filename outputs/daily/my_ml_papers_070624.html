<html><head></head><html><head><style>
/* Style the button that is used to open and close the collapsible content */
.collapsible {
  background-color: #eee;
  color: #444;
  cursor: pointer;
  padding: 18px;
  width: 100%;
  border: none;
  text-align: left;
  outline: none;
  font-size: 15px;
}

/* Add a background color to the button if it is clicked on (add the .active class with JS), and when you move the mouse over it (hover) */
.active, .collapsible:hover {
  background-color: #ccc;
}

/* Style the collapsible content. Note: hidden by default */
.content {
  padding: 0 18px;
  border: 5px ;
  margin: auto;
  width: 50%;
  padding: 10px;
}
</style></head><body><div class="intro content">
<h1>Recent Developments in Machine Learning Research</h1>
<p>Welcome to our newsletter, where we bring you the latest breakthroughs in machine learning research. In this edition, we will be highlighting some exciting new developments that have the potential to make a lasting impact in academic research. From novel architectures for capturing long-range dependencies on graphs to more efficient training methods for large language models, these papers showcase the continuous advancements in the field of machine learning. So let's dive in and explore the potential breakthroughs presented in these papers!</p> </div><div class="content">
<h4><a href=http://arxiv.org/abs/2406.03386v1>Learning Long Range Dependencies on Graphs via Random Walks (2406.03386v1)</a> </h4>
<p>
The paper presents a novel architecture, NeuralWalker, that combines random walks with message passing to overcome the limitations of existing methods in capturing long-range dependencies on graphs. This approach offers more expressive graph representations, the ability to utilize any sequence model, and flexibility in integrating various GNN and GT architectures. Experimental evaluations show significant performance improvements on 19 benchmark datasets, indicating potential for lasting impact in academic research.</p>
</div>
<div class="content">
<h4><a href=http://arxiv.org/abs/2406.03488v1>Seq1F1B: Efficient Sequence-Level Pipeline Parallelism for Large Language Model Training (2406.03488v1)</a> </h4>
<p>
The paper presents a new pipeline scheduling method, Seq1F1B, for training large language models (LLMs) on long sequences. This method reduces memory footprint and bubble size, resulting in higher training throughput compared to existing methods. It also allows for training LLMs with 30B parameters on sequences up to 64k without recomputation strategies. This has the potential to greatly impact academic research in the field of LLM training and scalability.</p>
</div>
<div class="content">
<h4><a href=http://arxiv.org/abs/2406.03368v1>IrokoBench: A New Benchmark for African Languages in the Age of Large Language Models (2406.03368v1)</a> </h4>
<p>
The paper introduces IrokoBench, a benchmark dataset for low-resource African languages, to evaluate the performance of large language models (LLMs). The evaluation reveals a significant performance gap between high-resource and low-resource languages, as well as between open and proprietary LLMs. The findings highlight the need for more efforts to develop and adapt LLMs for African languages, potentially creating a lasting impact in academic research.</p>
</div>
<div class="content">
<h4><a href=http://arxiv.org/abs/2406.03476v1>Does your data spark joy? Performance gains from domain upsampling at the end of training (2406.03476v1)</a> </h4>
<p>
This paper explores the potential benefits of using smaller, domain-specific datasets in addition to large, general web scrapes for pretraining large language models. By upsampling these smaller datasets at the end of training, significant performance gains can be achieved on difficult benchmarks. This technique allows for cost-effective experimentation with different pretraining datasets, potentially leading to lasting impacts in academic research on language models.</p>
</div>
<div class="content">
<h4><a href=http://arxiv.org/abs/2406.03496v1>Wings: Learning Multimodal LLMs without Text-only Forgetting (2406.03496v1)</a> </h4>
<p>
Wings is a novel multimodal large language model (MLLM) that addresses the issue of text-only forgetting in traditional MLLMs. By incorporating extra modules and complementary visual and textual learners, Wings excels in both text-only dialogues and multimodal comprehension. This has the potential to greatly impact academic research by improving the performance of MLLMs in various tasks, as demonstrated by its superior performance on a newly constructed benchmark.</p>
</div>
<div class="content">
<h4><a href=http://arxiv.org/abs/2406.03287v1>SpikeLM: Towards General Spike-Driven Language Modeling via Elastic Bi-Spiking Mechanisms (2406.03287v1)</a> </h4>
<p>
The paper presents SpikeLM, a fully spiking mechanism for general language tasks that addresses the limitations of existing spiking neural networks (SNNs) in encoding semantic information. This novel approach, with bi-directional, elastic amplitude, and elastic frequency encoding, shows promising results in achieving higher accuracy and bridging the performance gap between SNNs and artificial neural networks (ANNs) in language modeling. This has the potential to greatly impact academic research in the field of energy-efficient artificial intelligence and bio-inspired SNNs.</p>
</div>
<div class="content">
<h4><a href=http://arxiv.org/abs/2406.03280v1>FusionBench: A Comprehensive Benchmark of Deep Model Fusion (2406.03280v1)</a> </h4>
<p>
FusionBench is a comprehensive benchmark that evaluates the effectiveness and robustness of various deep model fusion techniques across a wide range of tasks. With 26 tasks, 74 models, and 16 fusion techniques, it provides a valuable resource for researchers to understand and replicate results in this emerging field. Its consistent expansion and well-documented resources have the potential to make a lasting impact in academic research of deep model fusion.</p>
</div>
<div class="content">
<h4><a href=http://arxiv.org/abs/2406.03445v1>Pre-trained Large Language Models Use Fourier Features to Compute Addition (2406.03445v1)</a> </h4>
<p>
This paper explores the mathematical reasoning capabilities of pre-trained large language models (LLMs) and reveals that they use Fourier features to compute basic arithmetic, such as addition. The authors demonstrate that pre-training is crucial for this mechanism and that it can significantly improve the accuracy of LLMs in performing algorithmic tasks. This has the potential to greatly impact academic research by unlocking the ability of LLMs to learn precise mechanisms for various tasks.</p>
</div>
<div class="content">
<h4><a href=http://arxiv.org/abs/2406.03339v1>The Challenges of Evaluating LLM Applications: An Analysis of Automated, Human, and LLM-Based Approaches (2406.03339v1)</a> </h4>
<p>
This paper discusses the challenges of evaluating chatbot applications and the need for effective evaluation methods. It introduces a comprehensive factored evaluation mechanism that can be used with both human and LLM-based evaluations. The results of an experimental evaluation using this mechanism show that it provides better insights for improving LLM applications and highlights the importance of human evaluation in critical areas. This has the potential to create a lasting impact in academic research by improving the evaluation process for chatbot applications.</p>
</div>
<div class="content">
<h4><a href=http://arxiv.org/abs/2406.03470v1>SpikeZIP-TF: Conversion is All You Need for Transformer-based SNN (2406.03470v1)</a> </h4>
<p>
The paper presents a new method, called SpikeZIP-TF, for converting artificial neural networks (ANN) to spiking neural networks (SNN) without any loss in accuracy. This method has the potential to greatly improve the performance of Transformer-based SNNs, which are currently lagging behind their ANN counterparts in terms of accuracy. The code for SpikeZIP-TF is publicly available, making it accessible for further research and potential lasting impact in the field of academic research.</p>
</div></body></html><body></body></html>