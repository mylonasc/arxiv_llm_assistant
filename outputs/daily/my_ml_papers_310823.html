<html><head></head><html><head><style>
/* Style the button that is used to open and close the collapsible content */
.collapsible {
  background-color: #eee;
  color: #444;
  cursor: pointer;
  padding: 18px;
  width: 100%;
  border: none;
  text-align: left;
  outline: none;
  font-size: 15px;
}

/* Add a background color to the button if it is clicked on (add the .active class with JS), and when you move the mouse over it (hover) */
.active, .collapsible:hover {
  background-color: #ccc;
}

/* Style the collapsible content. Note: hidden by default */
.content {
  padding: 0 18px;
  border: 5px ;
  margin: auto;
  width: 50%;
  padding: 10px;
}
</style></head><body><div class="intro content">
<h1>Unlocking the Potential of Machine Learning Research: Recent Developments</h1>

<p>The field of machine learning research is constantly evolving, with new developments and breakthroughs being made every day. From LM-Infinite's on-the-fly length generalization of Large Language Models (LLMs) to Jais and Jais-chat's improved knowledge and reasoning capabilities in Arabic, the potential for machine learning research to create a lasting impact is clear. This newsletter will present recent developments in machine learning research, from W4A8 post-training quantization methods to Generalized Referring Expression Comprehension (GREC) benchmarks, and explore the potential for these breakthroughs to create a lasting impact in academic research.</p> </div><div class="content">
<h4><a href=http://arxiv.org/abs/2308.16137v1>LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models (2308.16137v1)</a> </h4>
<p>
LM-Infinite is a simple yet effective solution for on-the-fly length generalization of Large Language Models (LLMs). It is computationally efficient and can generate fluent texts up to 32k tokens, with 2.72x decoding speedup. It also shows potential to create a lasting impact in academic research, as it can be applied to a variety of LLMs and can be used for downstream tasks on longer inputs than those used for training.</p>
</div>
<div class="content">
<h4><a href=http://arxiv.org/abs/2308.15987v1>FPTQ: Fine-grained Post-Training Quantization for Large Language Models (2308.15987v1)</a> </h4>
<p>
This paper presents a novel W4A8 post-training quantization method for large language models, which combines the advantages of both W8A8 and W4A16 recipes. The proposed method features layerwise activation quantization strategies and fine-grained weight quantization, and achieves state-of-the-art performance without further fine-tuning. This could create a lasting impact in academic research, enabling the deployment of large language models for real-world applications.</p>
</div>
<div class="content">
<h4><a href=http://arxiv.org/abs/2308.16149v1>Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models (2308.16149v1)</a> </h4>
<p>
Jais and Jais-chat are new open generative large language models (LLMs) for Arabic, based on the GPT-3 decoder-only architecture. With 13 billion parameters, they demonstrate improved knowledge and reasoning capabilities in Arabic compared to existing models, and are competitive in English despite being trained on less data. The models have the potential to create a lasting impact in academic research of Arabic LLMs.</p>
</div>
<div class="content">
<h4><a href=http://arxiv.org/abs/2308.15952v1>Benchmarking Multilabel Topic Classification in the Kyrgyz Language (2308.15952v1)</a> </h4>
<p>
This paper presents a new benchmark for multilabel topic classification in Kyrgyz, providing a dataset and baseline models for evaluation. The potential for this work to create a lasting impact in academic research is high, as it provides a valuable resource for the underrepresented language and opens up new possibilities for future research.</p>
</div>
<div class="content">
<h4><a href=http://arxiv.org/abs/2308.16118v1>Response: Emergent analogical reasoning in large language models (2308.16118v1)</a> </h4>
<p>
This paper suggests that large language models have the potential to create a lasting impact in academic research by providing zero-shot solutions to a broad range of analogy problems. However, the authors' experiments have yet to be replicated and further research is needed to determine if the potential benefits of these techniques can be realized.</p>
</div>
<div class="content">
<h4><a href=http://arxiv.org/abs/2308.15996v1>DTrOCR: Decoder-only Transformer for Optical Character Recognition (2308.15996v1)</a> </h4>
<p>
This paper presents a simpler and more effective method for text recognition, DTrOCR, which uses a decoder-only Transformer to take advantage of a pre-trained generative language model. Experiments show that DTrOCR outperforms current state-of-the-art methods in recognizing printed, handwritten, and scene text in both English and Chinese, with potential to create a lasting impact in academic research.</p>
</div>
<div class="content">
<h4><a href=http://arxiv.org/abs/2308.15982v1>MerA: Merging Pretrained Adapters For Few-Shot Learning (2308.15982v1)</a> </h4>
<p>
MerA is a new technique for few-shot learning that merges pretrained adapters to a single model, resulting in substantial improvements compared to single adapters and AdapterFusion. The proposed "same-track" setting further enhances the capacity of MerA, yielding impressive gains in academic research tasks.</p>
</div>
<div class="content">
<h4><a href=http://arxiv.org/abs/2308.16122v1>Spatial Graph Coarsening: Weather and Weekday Prediction with London's Bike-Sharing Service using GNN (2308.16122v1)</a> </h4>
<p>
This study demonstrated the potential of GNNs to create a lasting impact in academic research by introducing a new technique for predicting weather and weekday from bike-sharing data. The proposed Spatial Graph Coarsening operator and concatenation operator of graph features with trained node embeddings showed improved accuracy and cross-entropy loss compared to the baseline model.</p>
</div>
<div class="content">
<h4><a href=http://arxiv.org/abs/2308.16175v1>Quantifying Uncertainty in Answers from any Language Model via Intrinsic and Extrinsic Confidence Assessment (2308.16175v1)</a> </h4>
<p>
BSDetector is a method for detecting bad and speculative answers from a pretrained Large Language Model by estimating a numeric confidence score. This technique can be applied to any LLM accessible via a black-box API, and has the potential to create a lasting impact in academic research by providing a trustworthiness estimate for any LLM response. Experiments have shown that BSDetector can more accurately identify incorrect LLM responses than alternative uncertainty estimation procedures, and can also be used to obtain more accurate responses from the same LLM.</p>
</div>
<div class="content">
<h4><a href=http://arxiv.org/abs/2308.16182v1>GREC: Generalized Referring Expression Comprehension (2308.16182v1)</a> </h4>
<p>
This paper introduces a new benchmark, Generalized Referring Expression Comprehension (GREC), which extends the classic REC by allowing expressions to refer to any number of target objects. The proposed gRefCOCO dataset, a GREC method implementation code, and GREC evaluation code have the potential to create a lasting impact in academic research of the described techniques.</p>
</div></body></html><body></body></html>